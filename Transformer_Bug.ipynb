{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjo2mmmI1qOU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.cuda.amp import autocast\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerAE(torch.nn.Module):\n",
        "  def __init__(self, in_dim, d_model):\n",
        "    super(TransformerAE, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(in_dim, d_model)\n",
        "    self.transformer1 = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=8, batch_first=True)\n",
        "    self.transformer2 = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=8, batch_first=True)\n",
        "    self.linear3 = torch.nn.Linear(d_model, d_model//4)\n",
        "    self.linear4 = torch.nn.Linear(d_model//4, d_model)\n",
        "    self.transformer3 = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=8, batch_first=True)\n",
        "    self.transformer4 = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=8, batch_first=True)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.linear2 = torch.nn.Linear(d_model, in_dim)\n",
        "\n",
        "  def forward(self, x, m):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.transformer1(x, src_key_padding_mask=m)\n",
        "    x = self.transformer2(x, src_key_padding_mask=m)\n",
        "    x = self.linear3(x)\n",
        "    x = self.linear4(x)\n",
        "    x = self.transformer3(x, src_key_padding_mask=m)\n",
        "    x = self.transformer4(x, src_key_padding_mask=m)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "3d2CYw8G1zlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_sync_batchnorm(module):\n",
        "  print(\"Check module: \", module)\n",
        "  if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
        "    print(\"Find!: \", module)\n",
        "  for name, child in module.named_children():\n",
        "    convert_sync_batchnorm(child)"
      ],
      "metadata": {
        "id": "5KJYNbDg2kkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "model = TransformerAE(in_dim=10, d_model=128).to(device)\n",
        "model.eval()\n",
        "data = torch.rand((1, 10, 10))"
      ],
      "metadata": {
        "id": "McxKCs0C4ot2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_key_mask = torch.tensor([[False, False, False, False, False, False, False, False, False, True]]) #, True]])\n",
        "# torch.tensor([[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0]]) #\n",
        "output = model(data, src_key_mask)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaLPMWbM8Gku",
        "outputId": "7125c326-57f5-404a-f1ae-ebfde08f91e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.6527, -0.1148,  0.2900, -0.5350, -0.1129,  0.3372, -0.1269,\n",
            "           0.0983, -0.0913, -0.5693],\n",
            "         [ 0.5595, -0.3358,  0.0597, -0.4438, -0.2606,  0.1555, -0.2767,\n",
            "           0.0903,  0.1831, -0.5175],\n",
            "         [ 0.4743, -0.3141,  0.1776, -0.4160, -0.3058,  0.1261, -0.2394,\n",
            "          -0.0543,  0.2081, -0.5527],\n",
            "         [ 0.4412,  0.1224,  0.2447, -0.5129, -0.1260,  0.3266, -0.1891,\n",
            "          -0.0256, -0.2262, -0.5772],\n",
            "         [ 0.5774, -0.2624,  0.2801, -0.4644, -0.2591,  0.1187,  0.0482,\n",
            "          -0.0228, -0.0529, -0.5919],\n",
            "         [ 0.6015, -0.4583,  0.1572, -0.4470, -0.4000,  0.2482,  0.0099,\n",
            "          -0.1215, -0.6056, -0.6185],\n",
            "         [ 0.7189, -0.1084,  0.0248, -0.2436, -0.2130,  0.4159, -0.3751,\n",
            "          -0.0783, -0.2534, -0.6384],\n",
            "         [ 0.8287, -0.4687, -0.0795, -0.3876, -0.4481,  0.1981, -0.1872,\n",
            "          -0.0986, -0.2466, -0.6868],\n",
            "         [ 0.7980, -0.2646,  0.2422, -0.3622, -0.2688,  0.1366,  0.0313,\n",
            "          -0.0995, -0.1208, -0.6254],\n",
            "         [ 0.7626, -0.2567,  0.3415, -0.3009, -0.2626,  0.0711,  0.0788,\n",
            "          -0.0856,  0.0456, -0.5419]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_sync_batchnorm(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YYS82jn5B7M",
        "outputId": "d9d6547b-e7dd-42db-9795-bdc07c14700b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check module:  TransformerAE(\n",
            "  (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (transformer1): TransformerEncoderLayer(\n",
            "    (self_attn): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout1): Dropout(p=0.1, inplace=False)\n",
            "    (dropout2): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer2): TransformerEncoderLayer(\n",
            "    (self_attn): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout1): Dropout(p=0.1, inplace=False)\n",
            "    (dropout2): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (linear3): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (linear4): Linear(in_features=32, out_features=128, bias=True)\n",
            "  (transformer3): TransformerEncoderLayer(\n",
            "    (self_attn): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout1): Dropout(p=0.1, inplace=False)\n",
            "    (dropout2): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer4): TransformerEncoderLayer(\n",
            "    (self_attn): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout1): Dropout(p=0.1, inplace=False)\n",
            "    (dropout2): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "Check module:  Linear(in_features=128, out_features=128, bias=True)\n",
            "Check module:  TransformerEncoderLayer(\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "Check module:  MultiheadAttention(\n",
            "  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "Check module:  NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "Check module:  Linear(in_features=128, out_features=2048, bias=True)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Linear(in_features=2048, out_features=128, bias=True)\n",
            "Check module:  LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "Check module:  LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  TransformerEncoderLayer(\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "Check module:  MultiheadAttention(\n",
            "  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "Check module:  NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "Check module:  Linear(in_features=128, out_features=2048, bias=True)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Linear(in_features=2048, out_features=128, bias=True)\n",
            "Check module:  LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "Check module:  LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Linear(in_features=128, out_features=32, bias=True)\n",
            "Check module:  Linear(in_features=32, out_features=128, bias=True)\n",
            "Check module:  TransformerEncoderLayer(\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "Check module:  MultiheadAttention(\n",
            "  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "Check module:  NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "Check module:  Linear(in_features=128, out_features=2048, bias=True)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Linear(in_features=2048, out_features=128, bias=True)\n",
            "Check module:  LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "Check module:  LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  TransformerEncoderLayer(\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "Check module:  MultiheadAttention(\n",
            "  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "Check module:  NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "Check module:  Linear(in_features=128, out_features=2048, bias=True)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Linear(in_features=2048, out_features=128, bias=True)\n",
            "Check module:  LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "Check module:  LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  Dropout(p=0.1, inplace=False)\n",
            "Check module:  ReLU()\n",
            "Check module:  Linear(in_features=128, out_features=128, bias=True)\n"
          ]
        }
      ]
    }
  ]
}